# 2026-02-09 - Scholarship Bot Deep Crawl & Systems Review

## Scholarship Bot - Deep Crawl Solution

### Problema
- `discover_local.py` só encontrava 2 bolsas por universidade
- Precisamos de TODAS as bolsas (centenas por universidade)
- Objetivo: 5000+ bolsas no sistema

### Solução Criada
**Script:** `/home/ubuntu/projects/scholarship-bot/deep_crawl.py`
- MAX_DEPTH=5 (5-8 níveis de profundidade)
- MAX_PAGES=300 (até 2000 páginas por universidade)
- Múltiplos subdomínios: sfs, finaid, gsas, registrar, etc.
- Resultado: **483+ bolsas do MIT** (vs 2 antes)

### Codex Integration
**Doc:** `/home/ubuntu/projects/scholarship-bot/CODEX_INTEGRATION.md`
- Protocolo file-based: `/tmp/codex_task.json` → `/tmp/codex_result.json`
- Codex sandbox sem internet - OpenClaw como intermediário
- Tasks: `crawl_university`, `fetch_url`, `extract_scholarships`, `test_internet`
- Model: Gemini Flash Lite para operações de internet (tokens ilimitados)

### Bloqueio
- Deep crawl killed por timeout (180s limit)
- Precisa rodar com `nohup` ou background

## Jesus Sincero - Corrigido ✅

### Problema
- `posts_current.json` só tinha dados de 2026-02-05
- Nenhum post novo sendo gerado

### Solução
**Script:** `/home/ubuntu/clawd/sessions/personajes/scripts/batch-generator-v2.sh`
- Usa `openclaw sessions_spawn` (antes heredoc quebrado)
- Gerado 35 posts (2026-02-09 até 2026-02-15)
- Crons reativados e funcionando

## Job Curator - Pausado ⏸️

- 3 crons FREE pausados com prefixo `#PAUSED:`
- Aguardando foco em scholarship bot

## Próximos Passos
1. Executar deep_crawl com timeout maior (nohup/background)
2. Criar executor script para Codex integration
3. Replicar deep crawl para 150 universidades
4. Extrair detalhes: deadline, valor, eligibilidade

## Regras Críticas (Mestre definiu)
- ❌ NUNCA usar Gemini para Claude tasks
- ❌ NÃO usar Brave API para scholarship discovery
- ✅ Gemini Flash Lite OK para Codex internet ops
- ✅ Priorizar economia de tokens (Haiku > Opus)
