#!/usr/bin/env python3
"""
Job Curator Bot v2.1 - MAIN PIPELINE
Ciclo de 24h: Pesquisa ‚Üí Filtra ‚Üí Resolve ‚Üí Analisa ‚Üí Valida ‚Üí Posta (3x/dia)
"""

import os
import sys
import json
import logging
import argparse
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# Setup path
sys.path.insert(0, str(Path(__file__).parent))

from job_sources import collect_from_all_sources, categorize_sector
from job_filters import filter_jobs
from link_resolver import resolve_job_link
from job_analyzer import batch_analyze_jobs, JobAnalysis
from diversity_validator import validate_diversity, print_batch_summary
from telegram_poster import post_to_telegram, post_via_clawdbot_message
from cache_manager import cache_get, cache_set

# Config
STATE_FILE = Path(__file__).parent / "state.json"
LOG_DIR = Path(__file__).parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / f"{datetime.now().strftime('%Y-%m-%d')}.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class _RedactingFilter(logging.Filter):
    _token_re = re.compile(r"\b\d{9,}:[A-Za-z0-9_-]{20,}\b")

    def __init__(self):
        super().__init__()
        self._secrets = [
            os.getenv("TELEGRAM_BOT_TOKEN"),
            os.getenv("GEMINI_API_KEY"),
            os.getenv("GOOGLE_API_KEY"),
            os.getenv("TELEGRAM_GROUP_ID"),
            os.getenv("TELEGRAM_CHAT_ID"),
            os.getenv("ALLOWED_USER_ID"),
        ]
        self._secrets = [s for s in self._secrets if s]

    def _redact(self, text: str) -> str:
        if not text:
            return text
        text = self._token_re.sub("<redacted-token>", text)
        for secret in self._secrets:
            text = text.replace(secret, "<redacted>")
        return text

    def filter(self, record: logging.LogRecord) -> bool:
        msg = record.getMessage()
        redacted = self._redact(msg)
        record.msg = redacted
        record.args = ()
        return True


for _h in logging.getLogger().handlers:
    _h.addFilter(_RedactingFilter())


def load_state() -> dict:
    """Carrega estado (vagas j√° postadas, etc)"""
    if STATE_FILE.exists():
        try:
            return json.loads(STATE_FILE.read_text())
        except:
            return {}
    return {}


def save_state(state: dict):
    """Salva estado"""
    STATE_FILE.write_text(json.dumps(state, indent=2))


def run_daily_research(limit_per_source: int = 20) -> List[Dict[str, Any]]:
    """
    FASE 1: Pesquisa di√°ria (uma vez por dia)
    Coleta de m√∫ltiplas fontes, filtra, resolve links.
    """
    logger.info("="*60)
    logger.info("üìã FASE 1: PESQUISA DI√ÅRIA")
    logger.info("="*60)
    
    # Tenta cache (24h)
    cached, cache_info = cache_get("daily_research")
    if cached and not cache_info["expired"]:
        logger.info(f"‚úì Usando cache ({cache_info['age_hours']}h old)")
        return cached
    
    # Coleta das fontes
    logger.info("üîç Coletando vagas de m√∫ltiplas fontes...")
    raw_jobs = collect_from_all_sources(limit_per_source=limit_per_source)
    
    if not raw_jobs:
        logger.error("‚ùå Nenhuma vaga coletada!")
        return []
    
    logger.info(f"‚úì {len(raw_jobs)} vagas coletadas")
    
    # Filtra por pa√≠s/setor/idioma
    logger.info("\nüåç Filtrando por pa√≠s, setor, idioma...")
    filtered_jobs = filter_jobs(raw_jobs)
    logger.info(f"‚úì {len(filtered_jobs)} vagas ap√≥s filtros")
    
    if not filtered_jobs:
        logger.error("‚ùå Nenhuma vaga ap√≥s filtros!")
        return []
    
    # Resolve links (fonte ‚Üí site oficial)
    logger.info("\nüîó Resolvendo links para sites oficiais...")
    resolved_jobs = []
    
    for job in filtered_jobs:
        direct_url, status = resolve_job_link(job)
        
        if direct_url and status in ["direct", "resolved"]:
            job['_direct_url'] = direct_url
            job['_url_status'] = status
            resolved_jobs.append(job)
        else:
            logger.debug(f"‚ùå N√£o resolvido: {job['company']} - {job['title']}")
    
    logger.info(f"‚úì {len(resolved_jobs)} vagas com links diretos")
    
    if not resolved_jobs:
        logger.error("‚ùå Nenhuma vaga com link direto!")
        return []
    
    # Salva em cache por 24h
    cache_set("daily_research", resolved_jobs, ttl_hours=24)
    
    return resolved_jobs


def run_daily_analysis(jobs: List[Dict[str, Any]]) -> List[JobAnalysis]:
    """
    FASE 2: An√°lise com Claude (1 batch call)
    """
    logger.info("="*60)
    logger.info("üìä FASE 2: AN√ÅLISE COM CLAUDE")
    logger.info("="*60)
    
    if not jobs:
        logger.error("Nenhuma vaga para analisar")
        return []
    
    # Limita a 20 para an√°lise (m√°ximo eficiente por call)
    to_analyze = jobs[:20]
    
    logger.info(f"ü§ñ Analisando {len(to_analyze)} vagas em 1 batch call...")
    analyzed = batch_analyze_jobs(to_analyze)
    
    logger.info(f"‚úì {len(analyzed)} vagas aprovadas ap√≥s an√°lise")
    
    return analyzed


def run_daily_validation(analyzed_jobs: List[JobAnalysis]) -> List[JobAnalysis]:
    """
    FASE 3: Valida√ß√£o de diversidade
    Garante que as 3 vagas selecionadas t√™m diversidade.
    """
    logger.info("="*60)
    logger.info("‚úÖ FASE 3: VALIDA√á√ÉO DE DIVERSIDADE")
    logger.info("="*60)
    
    if len(analyzed_jobs) < 3:
        logger.error(f"‚ùå S√≥ {len(analyzed_jobs)} vagas aprovadas, precisa de 3 m√≠nimo!")
        logger.warning("   Tentando novamente amanh√£...")
        return []
    
    # Tenta diferentes combina√ß√µes at√© encontrar 3 com diversidade
    for i in range(min(10, len(analyzed_jobs) - 2)):
        batch = analyzed_jobs[i:i+3]
        
        valid, failing = validate_diversity(batch)
        
        if valid:
            logger.info("\n‚úÖ Batch validado com sucesso!")
            logger.info(print_batch_summary(batch))
            return batch
    
    logger.error("‚ùå N√£o conseguiu encontrar 3 vagas com diversidade garantida")
    logger.warning("   Tentando novamente amanh√£...")
    
    return []


def post_jobs(jobs: List[JobAnalysis], bot_token: str, chat_id: str) -> int:
    """
    FASE 4: Posting (pode ser chamado at√© 3x/dia)
    """
    logger.info("="*60)
    logger.info(f"üì§ POSTING: {len(jobs)} vaga(s)")
    logger.info("="*60)
    
    posted = 0
    
    for job in jobs:
        # Tenta com clawdbot message (mais simples)
        success = post_via_clawdbot_message(job, channel="telegram")
        
        if not success and bot_token and chat_id:
            # Fallback: API direto
            success = post_to_telegram(job, bot_token, chat_id)
        
        if success:
            posted += 1
    
    return posted


def main():
    parser = argparse.ArgumentParser(description="Job Curator Bot v2.1")
    parser.add_argument("--mode", choices=["research", "analyze", "validate", "post"], 
                       default="full", help="Modo execu√ß√£o")
    parser.add_argument("--dry-run", action="store_true", help="Simula sem postar")
    parser.add_argument("--bot-token", help="Telegram bot token")
    parser.add_argument("--chat-id", help="Telegram chat ID")
    parser.add_argument("--limit", type=int, default=20, help="Vagas por fonte")
    
    args = parser.parse_args()
    
    bot_token = args.bot_token or os.environ.get("TELEGRAM_BOT_TOKEN")
    chat_id = args.chat_id or os.environ.get("TELEGRAM_CHAT_ID")
    
    logger.info("\n" + "="*60)
    logger.info("üöÄ JOB CURATOR BOT v2.1")
    logger.info(f"   Timestamp: {datetime.now()}")
    logger.info("="*60)
    
    # PESQUISA (uma vez por dia, 00:00 UTC)
    logger.info("\nüìã Pesquisa Di√°ria...")
    research_jobs = run_daily_research(limit_per_source=args.limit)
    
    if not research_jobs:
        logger.error("‚ùå Falha na pesquisa. Abortando.")
        return 1
    
    # AN√ÅLISE (uma vez por dia, logo ap√≥s pesquisa)
    logger.info("\nüìä An√°lise...")
    analyzed_jobs = run_daily_analysis(research_jobs)
    
    if not analyzed_jobs:
        logger.error("‚ùå Falha na an√°lise. Abortando.")
        return 1
    
    # VALIDA√á√ÉO (uma vez por dia)
    logger.info("\n‚úÖ Valida√ß√£o...")
    validated_jobs = run_daily_validation(analyzed_jobs)
    
    if not validated_jobs:
        logger.error("‚ùå Falha na valida√ß√£o. Tentando amanh√£.")
        return 1
    
    # POSTING (pode ser 3x/dia)
    if not args.dry_run:
        logger.info("\nüì§ Posting...")
        posted = post_jobs(validated_jobs, bot_token, chat_id)
        logger.info(f"‚úÖ {posted}/{len(validated_jobs)} vagas postadas")
    else:
        logger.info("\n[DRY RUN] Vagas prontas para posting:")
        for job in validated_jobs:
            logger.info(f"   ‚Ä¢ {job.titulo} @ {job.empresa}")
    
    logger.info("\n" + "="*60)
    logger.info("‚úÖ PIPELINE COMPLETO")
    logger.info("="*60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
